{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM SDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djrl10/trying/blob/master/LSTM_SDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKb7F-_vcuJY"
      },
      "source": [
        "Source:\n",
        "https://github.com/c2nes/javalang\n",
        "Software Defect Prediction via ConvolutionalNeural Network\n",
        "Seml: A Semantic LSTM Model for Software Defect Prediction\n",
        "Automatically Learning Semantic Features for Defect Prediction\n",
        "https://github.com/kevinqiu1990/TCNN\n",
        "Transfer Convolutional Neural Network for Cross-Project Defect Prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBmWhtqdj54P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a41593-04dc-467e-9245-ae2021bb2dc3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNrh0NzlcuJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1692c9b5-54d5-43ed-d6a6-07a71de4bdcb"
      },
      "source": [
        "!pip3 install javalang\n",
        "import tensorflow as tf\n",
        "import javalang\n",
        "import javalang.tree as jlt\n",
        "from os import walk\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "import os\n",
        "import glob\n",
        "import re , itertools\n",
        "from html import unescape\n",
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from string import punctuation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting javalang\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/e0/12344443d66b9a84844171be90112892a371da6db09866741774b8bc0a2f/javalang-0.13.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from javalang) (1.15.0)\n",
            "Installing collected packages: javalang\n",
            "Successfully installed javalang-0.13.0\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.2.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ptbEBfcuJp"
      },
      "source": [
        "1)Method invocation and class instance creation nodes:\n",
        "We extract and record these nodes using their method or\n",
        "class names. For example, we extract method pop() and\n",
        "push() in File2.java in Figure 1 and record them as pop\n",
        "and push.\n",
        "\n",
        "2)Declaration nodes: We extract method declaration, type\n",
        "declaration and enum declaration nodes, and record them\n",
        "using their names.\n",
        "\n",
        "3)Control-flow nodes: We extract those statements or\n",
        "clauses related to control flow of a program, e.g., if state-\n",
        "ment, for statement, while statement, catch clause.\n",
        "Control-flow nodes are recorded using their statement types,\n",
        "e.g., if statement is recorded as if, and catch clause is recorded\n",
        "as catch.\n",
        "Therefore\n",
        "\n",
        "karena hanya 3 aturan yg digunakan maka types node dari AST yg digunakan adalah dibawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3s2FoDXcuJr"
      },
      "source": [
        "types = [jlt.FormalParameter, jlt.BasicType, jlt.PackageDeclaration, jlt.InterfaceDeclaration, jlt.CatchClauseParameter,\n",
        "         jlt.ClassDeclaration, jlt.MethodInvocation, jlt.SuperMethodInvocation, jlt.MemberReference, jlt.SuperMemberReference,\n",
        "         jlt.ConstructorDeclaration, jlt.ReferenceType, jlt.MethodDeclaration, jlt.VariableDeclarator, jlt.IfStatement,\n",
        "         jlt.WhileStatement, jlt.DoStatement, jlt.ForStatement, jlt.AssertStatement, jlt.BreakStatement,\n",
        "         jlt.ContinueStatement, jlt.ReturnStatement, jlt.ThrowStatement, jlt.SynchronizedStatement, jlt.TryStatement,\n",
        "         jlt.SwitchStatement, jlt.BlockStatement, jlt.StatementExpression, jlt.TryResource, jlt.CatchClause,\n",
        "         jlt.CatchClauseParameter, jlt.SwitchStatementCase, jlt.ForControl, jlt.EnhancedForControl]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF5HKq1kcuJ2"
      },
      "source": [
        "Function untuk parse dari source code ke AST -> Token\n",
        "\n",
        "Didefinisikan secara manual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nw0FeyEcuJ4",
        "scrolled": true
      },
      "source": [
        "def parse(source):\n",
        "    tokens = javalang.parse.parse(source)\n",
        "    result = []\n",
        "    for path, node in tokens:\n",
        "        if isinstance(node, jlt.PackageDeclaration):\n",
        "            result.append(node.name)\n",
        "            continue\n",
        "        if isinstance(node, jlt.IfStatement):\n",
        "            result.append(\"if\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.InterfaceDeclaration):\n",
        "            result.append(node.name)\n",
        "            continue\n",
        "        if isinstance(node, jlt.ClassDeclaration):\n",
        "            result.append(node.name)\n",
        "            continue\n",
        "        if isinstance(node, jlt.MethodInvocation):\n",
        "            result.append(node.member)\n",
        "            continue\n",
        "        if isinstance(node, jlt.MemberReference):\n",
        "            result.append(node.member)\n",
        "            continue\n",
        "        if isinstance(node, jlt.ConstructorDeclaration):\n",
        "            result.append(node.name)\n",
        "            continue\n",
        "        if isinstance(node, jlt.ReferenceType):\n",
        "            result.append(node.name)\n",
        "            continue\n",
        "        if isinstance(node, jlt.MethodDeclaration):\n",
        "            result.append(node.name)\n",
        "            continue\n",
        "        if isinstance(node, jlt.VariableDeclarator):\n",
        "            result.append(node.name)\n",
        "            continue\n",
        "        if isinstance(node, jlt.ThrowStatement):\n",
        "            result.append(\"throw\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.TryStatement):\n",
        "            result.append(\"try\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.BlockStatement):\n",
        "            result.append(\"block\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.CatchClause):\n",
        "            result.append(\"catch\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.ForStatement):\n",
        "            result.append(\"for\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.AssertStatement):\n",
        "            result.append(\"assert\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.BreakStatement):\n",
        "            result.append(\"break\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.ContinueStatement):\n",
        "            result.append(\"cotinue\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.ReturnStatement):\n",
        "            result.append(\"return\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.ReturnStatement):\n",
        "            result.append(\"return\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.ThrowStatement):\n",
        "            result.append(\"throw\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.SwitchStatement):\n",
        "            result.append(\"switch\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.WhileStatement):\n",
        "            result.append(\"while\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.DoStatement):\n",
        "            result.append(\"do\")\n",
        "            continue\n",
        "        if isinstance(node, jlt.CatchClauseParameter):\n",
        "            result.append(node.types)\n",
        "            continue\n",
        "        if isinstance(node, jlt.BasicType):\n",
        "            result.append(node.name)\n",
        "            continue\n",
        "        if isinstance(node, jlt.SynchronizedStatement):\n",
        "            result.append(\"Synchronized\")\n",
        "            continue\n",
        "    return result\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP56Dbc3cuJ-"
      },
      "source": [
        "Proses mengubah Source Code (File) -> AST -> Token\n",
        "\n",
        "Arsitektur Folder\n",
        "\n",
        "\n",
        "\"Nama Project\"\n",
        "\n",
        "    \"Defect\"\n",
        "\n",
        "        \"File Defect 1\"\n",
        "\n",
        "        \"File Defect 2\"\n",
        "\n",
        "        ....\n",
        "\n",
        "    \"Non-Defect\"\n",
        "\n",
        "        \"File Non-Defect 1\"\n",
        "\n",
        "        \"File Non-Defect 2\"\n",
        "\n",
        "        ...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSKdxJpNcuJ_",
        "scrolled": true
      },
      "source": [
        "def Tokenize(dir_name):\n",
        "    df_list = []\n",
        "    df = pd.DataFrame(columns=['File','Token','Defect'])\n",
        "    for (dirpath, dirnames, filenames) in walk(dir_name):\n",
        "        for label in dirnames: \n",
        "            for (dirpath, dirnames, filenames) in walk(dir_name+'/'+label):\n",
        "                for filename in filenames:\n",
        "                    f = open(dir_name+'/'+label+'/'+filename, \"r\")\n",
        "                    f = f.read()\n",
        "                    try:\n",
        "                        p = parse(f)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                    temp = pd.DataFrame([[filename,p,label]],columns=['File','Token','Defect'])\n",
        "                    df_list.append(temp)\n",
        "                break\n",
        "        break\n",
        "    df = pd.concat(df_list)\n",
        "    df = df.reset_index()\n",
        "    return df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0Lm8rlEe1WP"
      },
      "source": [
        "list_label = ['Tokencamel-1_6', 'Tokencamel-1_4']\n",
        "list_path = ['/content/drive/My Drive/data_set/camel-1.6', '/content/drive/My Drive/data_set/camel-1.4']\n",
        "datasets = dict()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZfsIVtTcuKH"
      },
      "source": [
        "for index in range(2) :\n",
        "  data = Tokenize(list_path[index])\n",
        "  datasets.update({list_label[index] : data})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haSC4ygcDAsK"
      },
      "source": [
        "features = dict()\n",
        "labels = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJqbPN_OuFBU"
      },
      "source": [
        "for key in datasets.keys() :\n",
        "  list_token = datasets[key].iloc[:, 2:-1]\n",
        "  label = datasets[key].iloc[:, -1:].values\n",
        "  features.update({key : list_token})\n",
        "  labels.update({key : label})\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIYdH0KcnLc7"
      },
      "source": [
        "clean_features = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c33SbDjHmtSU"
      },
      "source": [
        "for key in datasets.keys() :\n",
        "  feature = features[key].values\n",
        "  clean_feature = list()\n",
        "  for idx, val in enumerate(feature) :\n",
        "    filtered_feature = [x for x in val[0] if isinstance(x, str)]\n",
        "    clean_feature.append(filtered_feature)\n",
        "  clean_features.update({key : clean_feature})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGXKirR6qDwh"
      },
      "source": [
        "join_features = dict()\n",
        "join_label = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ2SMFK9pvaJ"
      },
      "source": [
        "for key in datasets.keys() :\n",
        "  lb = pd.DataFrame(labels[key], columns=['labels'])\n",
        "  lb_dum = pd.get_dummies(lb.labels)\n",
        "  lb_dum.head()\n",
        "  label = lb_dum[['Defect', 'Non-Defect']].values\n",
        "  join_label.update({key : label})\n",
        "  new_feature = [', '.join(item) for item in clean_features[key]]\n",
        "  join_features.update({key : new_feature})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEpObpLw-NzR"
      },
      "source": [
        "#Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN5oV2s3K3F9"
      },
      "source": [
        "cv = CountVectorizer()\n",
        "tfidf = TfidfTransformer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCgtvapTK6M3"
      },
      "source": [
        "join_features[key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmejIaZnNqKh"
      },
      "source": [
        "join_label[key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdxmk8j780WR"
      },
      "source": [
        "# Split data into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpX0iklWqzZa"
      },
      "source": [
        "token_trains = dict()\n",
        "token_tests = dict()\n",
        "label_trains = dict()\n",
        "label_tests = dict()\n",
        "for key in datasets.keys():\n",
        "  tkn_train, tkn_test, lbl_train, lbl_test = train_test_split(join_features[key], join_label[key], test_size=0.2, shuffle=True)\n",
        "  token_trains.update({key:tkn_train})\n",
        "  token_tests.update({key:tkn_test})\n",
        "  label_trains.update({key:lbl_train})\n",
        "  label_tests.update({key:lbl_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvNio9QsvIs6"
      },
      "source": [
        "print(len(token_trains['Tokencamel-1_4']))\n",
        "print(len(token_tests['Tokencamel-1_4']))\n",
        "print(len(label_trains['Tokencamel-1_4']))\n",
        "print(len(label_tests['Tokencamel-1_4']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9TW1pnvszx9"
      },
      "source": [
        "# tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded_trains = dict()\n",
        "padded_tests = dict()\n",
        "\n",
        "for key in datasets.keys():\n",
        "  tokenizer = Tokenizer(num_words=5000, oov_token='x', filters='!\"#$%&()*+,-./:;<=>@[\\]^_`{|}~ ')\n",
        "  tokenizer.fit_on_texts(token_trains[key]) \n",
        "  tokenizer.fit_on_texts(token_tests[key])\n",
        " \n",
        "  sekuens_train = tokenizer.texts_to_sequences(token_trains[key])\n",
        "  sekuens_test = tokenizer.texts_to_sequences(token_tests[key])\n",
        " \n",
        "  padded_train = pad_sequences(sekuens_train) \n",
        "  padded_test = pad_sequences(sekuens_test)\n",
        "\n",
        "  padded_tests.update({key : padded_test})\n",
        "  padded_trains.update({key : padded_train})\n",
        "\n",
        "  print(padded_test.shape)\n",
        "  print(padded_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t1jdj5M8p-0"
      },
      "source": [
        "#Model LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHepwuEDsz8t"
      },
      "source": [
        "# model\n",
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(2, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', metrics=['accuracy'], loss='binary_crossentropy',)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6V3in3ys5k_"
      },
      "source": [
        "# model fit\n",
        "results = dict()\n",
        "for key in datasets.keys():\n",
        "  history = model.fit(padded_trains[key], label_trains[key], epochs=5, \n",
        "                    steps_per_epoch = 30, validation_data=(padded_tests[key], label_tests[key]), verbose=2, \n",
        "                    validation_steps=30)\n",
        "  results.update({key:history})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBakJDA44xJM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COKYpfB9HGTT"
      },
      "source": [
        "for key in datasets.keys():  \n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqdJKR5IHH-a"
      },
      "source": [
        "# plot of loss\n",
        "for key in datasets.keys(): \n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4wSmLCA0C7X"
      },
      "source": [
        "#Model RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuqheskL8neJ"
      },
      "source": [
        "# model\n",
        "import tensorflow as tf\n",
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),\n",
        "    tf.keras.layers.SimpleRNN(128),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(2, activation='sigmoid')\n",
        "])\n",
        "model2.compile(optimizer='adam', metrics=['accuracy'], loss='binary_crossentropy',)\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmgUL_mow8Qp"
      },
      "source": [
        "# model fit\n",
        "results = dict()\n",
        "for key in datasets.keys():\n",
        "  history = model2.fit(padded_trains[key], label_trains[key], epochs=5, \n",
        "                    steps_per_epoch = 30, validation_data=(padded_tests[key], label_tests[key]), verbose=2, \n",
        "                    validation_steps=30)\n",
        "  results.update({key:history})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlCcZoJcy1Gj"
      },
      "source": [
        "for key in datasets.keys():  \n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2IQNzG7y7fC"
      },
      "source": [
        "# plot of loss\n",
        "for key in datasets.keys(): \n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}